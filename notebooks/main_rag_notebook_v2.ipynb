{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvxwfBPgkaP7"
      },
      "source": [
        "# Hierarchical RAG System\n",
        "\n",
        "Bu notebook, Project Gutenberg'den \"The Children of the New Forest\" kitabını kullanarak hiyerarşik parçalama yöntemiyle bir RAG (Retrieval-Augmented Generation) sistemi oluşturur.\n",
        "\n",
        "**Proje Detayları:**\n",
        "- **Kitap:** The Children of the New Forest by Frederick Marryat\n",
        "- **Release:** May 21, 2007\n",
        "- **Dataset:** NarrativeQA\n",
        "- **Chunking:** LlamaIndex HierarchicalNodeParser\n",
        "- **Vector DB:** Milvus Lite\n",
        "- **Embedding Model:** BAAI/bge-large-en-v1.5 (1024 dim)\n",
        "- **Retrieval:** AutoMergingRetriever (parent-child hierarchy)\n",
        "- **LLM:** google/gemma-3-1b-it\n",
        "- **Metrikler:** BLEU, ROUGE-1, ROUGE-2, ROUGE-L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwBRKABQkaP7"
      },
      "source": [
        "---\n",
        "## 1. Kurulum ve Hazırlık"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7uAtdr7kaP8"
      },
      "source": [
        "### 1.1 Kütüphaneleri Yükle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq6cQ3WbkaP8",
        "outputId": "32f7e1ab-422b-462d-f3ec-8a7e9de95f58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'V-RAG-Final'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Counting objects: 100% (112/112), done.\u001b[K\n",
            "remote: Compressing objects: 100% (112/112), done.\u001b[K\n",
            "remote: Total 112 (delta 54), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (112/112), 690.60 KiB | 6.06 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n",
            "/content/V-RAG-Final\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Git clone ve requirements\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Google Colab için\n",
        "if 'google.colab' in sys.modules:\n",
        "    # Repo'yu klonla veya güncelle\n",
        "    if not os.path.exists('V-RAG-Final'):\n",
        "        !git clone https://github.com/sendayildirim/V-RAG-Final\n",
        "    else:\n",
        "        %cd V-RAG-Final\n",
        "        !git pull\n",
        "        %cd ..\n",
        "\n",
        "    %cd V-RAG-Final\n",
        "\n",
        "    # Tüm requirements'ı\n",
        "    !pip install -q -r requirements.txt\n",
        "\n",
        "    sys.path.append('/content/V-RAG-Final/src')\n",
        "else:\n",
        "    # Local için\n",
        "    sys.path.append('/Users/senda.yildirim/Desktop/V-RAG-Final/src')\n",
        "    print(\"Local environment - requirements.txt'i manuel yükleyin: pip install -r requirements.txt\")\n",
        "\n",
        "print(\"Kütüphaneler yüklendi!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADriirj1kaP8"
      },
      "source": [
        "### 1.2 Gerekli Modülleri İçe Aktar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAUNKomgkaP8"
      },
      "outputs": [],
      "source": [
        "from data_loader import DataLoader\n",
        "from chunker_v2 import HierarchicalChunker\n",
        "from vector_store_v2 import VectorStore\n",
        "from rag_pipeline_v2 import RAGPipeline\n",
        "from baseline_model_v2 import BaselineModel\n",
        "from metrics import MetricsEvaluator\n",
        "from experiment_runner_v2 import ExperimentRunner\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import torch\n",
        "\n",
        "\n",
        "print(\"Modüller yüklendi!\")\n",
        "print(f\"GPU kullanılabilme durumu: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEvmNRR-kaP8"
      },
      "source": [
        "### 1.3 Hugging Face Login (Gemma-3 için)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2SymmVokaP8"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlU6tfFxkaP8"
      },
      "source": [
        "---\n",
        "## 2. Veri Hazırlama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fjsZNzYkaP9"
      },
      "source": [
        "### 2.1 Kitap ve Soruları İndir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fV7X2HhkaP9"
      },
      "outputs": [],
      "source": [
        "loader = DataLoader(data_dir=\"data\")\n",
        "data_paths = loader.load_all_data()\n",
        "\n",
        "print(\"\\nİndirilen dosyalar:\")\n",
        "print(f\"  Book: {data_paths['book']}\")\n",
        "print(f\"  Test: {data_paths['test']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivnf-Dg5kaP9"
      },
      "source": [
        "### 2.2 Test Verilerini Yükle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPfYuamUkaP9"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(data_paths['test'])\n",
        "\n",
        "print(f\"Toplam test sorusu: {len(test_df)}\")\n",
        "print(\"\\nİlk 3 soru:\")\n",
        "test_df[['question', 'answer1', 'answer2']].head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGwtxWv3kaP9"
      },
      "source": [
        "---\n",
        "## 3. Hiyerarşik Chunking (LlamaIndex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJG_wqtpkaP9"
      },
      "source": [
        "### 3.1 HierarchicalNodeParser ile Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pju7aU9PkaP9"
      },
      "outputs": [],
      "source": [
        "with open(data_paths['book'], 'r', encoding='utf-8') as f:\n",
        "    book_text = f.read()\n",
        "\n",
        "print(f\"Kitap uzunluğu: {len(book_text)} karakter\")\n",
        "\n",
        "chunker = HierarchicalChunker(\n",
        "    parent_size=2048,\n",
        "    child_size=512,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "\n",
        "nodes, node_mapping = chunker.chunk_text(book_text)\n",
        "\n",
        "stats = chunker.get_chunk_stats(nodes)\n",
        "print(\"\\nNode İstatistikleri:\")\n",
        "for key, value in stats.items():\n",
        "    if isinstance(value, dict):\n",
        "        print(f\"  {key}: {value}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value:.1f}\" if isinstance(value, float) else f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEleqhoMkaP9"
      },
      "source": [
        "### 3.2 Node Yapısını İncele"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJvwUWN9kaP9"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import get_leaf_nodes\n",
        "\n",
        "leaf_nodes = get_leaf_nodes(nodes)\n",
        "\n",
        "sample_child = leaf_nodes[0]\n",
        "print(\"Örnek Child Node:\")\n",
        "print(f\"  Node ID: {sample_child.node_id}\")\n",
        "print(f\"  Chapter: {sample_child.metadata.get('chapter')}\")\n",
        "print(f\"  Chapter Title: {sample_child.metadata.get('chapter_title')}\")\n",
        "print(f\"  Text uzunluğu: {len(sample_child.text)} karakter\")\n",
        "print(f\"  Text (ilk 200 karakter): {sample_child.text[:200]}...\")\n",
        "\n",
        "if hasattr(sample_child, 'parent_node') and sample_child.parent_node:\n",
        "    parent_id = sample_child.parent_node.node_id\n",
        "    print(f\"\\n  Parent Node ID: {parent_id}\")\n",
        "\n",
        "    if parent_id in node_mapping:\n",
        "        parent_node = node_mapping[parent_id]\n",
        "        print(f\"  Parent text uzunluğu: {len(parent_node.text)} karakter\")\n",
        "        print(f\"  Parent text (ilk 100 karakter): {parent_node.text[:100]}...\")\n",
        "    else:\n",
        "        print(f\"  Parent node mapping'de bulunamadı\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSsrvPzRkaP9"
      },
      "source": [
        "---\n",
        "## 4. Vector Store (Milvus + bge-large)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeULKSbMkaP9"
      },
      "source": [
        "### 4.1 Milvus Vector Store Oluştur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgeMhzkzkaP9"
      },
      "outputs": [],
      "source": [
        "vector_store = VectorStore(\n",
        "    db_path=\"./milvus_rag.db\",\n",
        "    model_name=\"BAAI/bge-large-en-v1.5\"\n",
        ")\n",
        "\n",
        "print(f\"Embedding boyutu: {vector_store.embedding_dim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDVYAQU2kaP9"
      },
      "source": [
        "### 4.2 Node'ları İndeksle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbPHh2TrkaP9"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "vector_store.create_index(nodes, node_mapping)\n",
        "indexing_time = time.time() - start\n",
        "\n",
        "print(f\"\\nToplam indexing süresi: {indexing_time:.2f}s\")\n",
        "\n",
        "stats = vector_store.get_stats()\n",
        "print(f\"\\nVector Store Stats:\")\n",
        "for key, value in stats.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osUgsC7dkaP9"
      },
      "source": [
        "### 4.3 AutoMergingRetriever Testi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bdpqft-IkaP9"
      },
      "outputs": [],
      "source": [
        "test_query = \"What is the title of this story?\"\n",
        "\n",
        "_, results = vector_store.hybrid_search(\n",
        "    query=test_query,\n",
        "    top_parents=3\n",
        ")\n",
        "\n",
        "print(f\"Test sorusu: {test_query}\")\n",
        "print(f\"\\nAutoMerging sonuçları ({len(results)} node):\")\n",
        "for i, result in enumerate(results, 1):\n",
        "    node_type = \"PARENT\" if result.get('is_parent', False) else \"CHILD\"\n",
        "    print(f\"\\n{i}. {node_type} (Chapter {result['chapter']}, Score: {result['score']:.4f})\")\n",
        "    print(f\"   Text: {result['text'][:150]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omQSuW2AkaP9"
      },
      "source": [
        "---\n",
        "## 5. Baseline Model (RAG'sız)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPBh9SyXkaP9"
      },
      "source": [
        "### 5.1 Baseline Model Oluştur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwXHi9rHkaP9"
      },
      "outputs": [],
      "source": [
        "baseline = BaselineModel(model_name=\"google/gemma-3-1b-it\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ5CbFZskaP-"
      },
      "source": [
        "### 5.2 Baseline ile Test Soruları"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE1IdK_kkaP-"
      },
      "outputs": [],
      "source": [
        "from performance_monitor import PeakMemoryMonitor\n",
        "import time\n",
        "\n",
        "questions = test_df['question'].tolist()\n",
        "\n",
        "baseline_monitor = PeakMemoryMonitor()\n",
        "baseline_monitor.record()\n",
        "\n",
        "print(\"Baseline model ile sorular cevaplanıyor...\")\n",
        "\n",
        "start_time = time.time()\n",
        "baseline_results = baseline.batch_answer_questions(\n",
        "    questions,\n",
        "    max_new_tokens=100,\n",
        "    memory_monitor=baseline_monitor\n",
        ")\n",
        "baseline_inference_time = time.time() - start_time\n",
        "\n",
        "baseline_memory_snapshot = baseline_monitor.record()\n",
        "baseline_memory_summary = baseline_monitor.summary()\n",
        "baseline_memory_used = baseline_memory_summary['memory_used_mb']\n",
        "baseline_initial_memory = baseline_memory_summary['initial_memory_mb']\n",
        "baseline_peak_memory = baseline_memory_summary['peak_memory_mb']\n",
        "baseline_end_memory = baseline_memory_snapshot.current_mb\n",
        "\n",
        "print(f\"{len(baseline_results)} soru cevaplandı!\")\n",
        "\n",
        "# Metrics\n",
        "print(\"\" + \"=\"*60)\n",
        "print(\"BASELINE MODEL PERFORMANS METRİKLERİ\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Toplam Inference Time: {baseline_inference_time:.2f} saniye\")\n",
        "print(f\"Ortalama Soru Başı Süre: {baseline_inference_time/len(questions):.2f} saniye\")\n",
        "print(f\"Memory Kullanımı (Peak - Initial): {baseline_memory_used:.2f} MB\")\n",
        "print(f\"Başlangıç Memory: {baseline_initial_memory:.2f} MB\")\n",
        "print(f\"Peak Memory: {baseline_peak_memory:.2f} MB\")\n",
        "print(f\"Bitiş Memory: {baseline_end_memory:.2f} MB\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"Örnek Baseline Cevaplar:\")\n",
        "for i, result in enumerate(baseline_results[:3], 1):\n",
        "    print(f\"{i}. Soru: {result['question']}\")\n",
        "    print(f\"   Cevap: {result['answer']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF2PO7UBkaP-"
      },
      "outputs": [],
      "source": [
        "baseline_df = pd.DataFrame(baseline_results)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "baseline_df.to_csv(\"results/baseline_QA.csv\", index=False)\n",
        "print(\"Baseline sonuçları kaydedildi: results/baseline_QA.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwFSArVFkaP-"
      },
      "source": [
        "---\n",
        "## 6. RAG Pipeline (AutoMergingRetriever)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_p4jaOrkaP-"
      },
      "source": [
        "### 6.1 RAG Pipeline Oluştur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TVMFeQUkaP-"
      },
      "outputs": [],
      "source": [
        "rag_pipeline = RAGPipeline(\n",
        "    vector_store=vector_store,\n",
        "    model_name=\"google/gemma-3-1b-it\",\n",
        "    temperature=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFLJVDWMkaP-"
      },
      "source": [
        "### 6.2 RAG ile Test Soruları"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaIGMGBMkaP-"
      },
      "outputs": [],
      "source": [
        "from performance_monitor import PeakMemoryMonitor\n",
        "import time\n",
        "\n",
        "rag_monitor = PeakMemoryMonitor()\n",
        "rag_monitor.record()\n",
        "\n",
        "print(\"RAG pipeline ile sorular cevaplanıyor...\")\n",
        "\n",
        "start_time = time.time()\n",
        "rag_results = rag_pipeline.batch_answer_questions(\n",
        "    questions,\n",
        "    top_k=3,\n",
        "    max_new_tokens=100,\n",
        "    memory_monitor=rag_monitor\n",
        ")\n",
        "rag_inference_time = time.time() - start_time\n",
        "\n",
        "rag_memory_snapshot = rag_monitor.record()\n",
        "rag_memory_summary = rag_monitor.summary()\n",
        "rag_memory_used = rag_memory_summary['memory_used_mb']\n",
        "rag_initial_memory = rag_memory_summary['initial_memory_mb']\n",
        "rag_peak_memory = rag_memory_summary['peak_memory_mb']\n",
        "rag_end_memory = rag_memory_snapshot.current_mb\n",
        "\n",
        "print(f\"{len(rag_results)} soru cevaplandı!\")\n",
        "\n",
        "# Metrics\n",
        "print(\"=\"*60)\n",
        "print(\"RAG PIPELINE PERFORMANS METRİKLERİ\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Toplam Inference Time: {rag_inference_time:.2f} saniye\")\n",
        "print(f\"Ortalama Soru Başı Süre: {rag_inference_time/len(questions):.2f} saniye\")\n",
        "print(f\"Memory Kullanımı (Peak - Initial): {rag_memory_used:.2f} MB\")\n",
        "print(f\"Başlangıç Memory: {rag_initial_memory:.2f} MB\")\n",
        "print(f\"Peak Memory: {rag_peak_memory:.2f} MB\")\n",
        "print(f\"Bitiş Memory: {rag_end_memory:.2f} MB\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"Örnek RAG Cevaplar:\")\n",
        "for i, result in enumerate(rag_results[:3], 1):\n",
        "    print(f\"{i}. Soru: {result['question']}\")\n",
        "    print(f\"   Cevap: {result['answer']}\")\n",
        "    print(f\"   Context (ilk 100 karakter): {result['context'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4puvMwR1HHo_"
      },
      "outputs": [],
      "source": [
        "rag_results_df = pd.DataFrame(rag_results)\n",
        "rag_results_df\n",
        "rag_results_df.to_csv(\"results/RAG_QA.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2hbtn8ezg-Q"
      },
      "source": [
        "### 6.3 Performans Metrikleri Karşılaştırması"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asvRNSA-zg-Q"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    baseline_inference = globals().get('baseline_inference_time', 'N/A')\n",
        "    baseline_mem = globals().get('baseline_memory_used', 'N/A')\n",
        "    rag_inference = globals().get('rag_inference_time', 'N/A')\n",
        "    rag_mem = globals().get('rag_memory_used', 'N/A')\n",
        "\n",
        "    perf_comparison = pd.DataFrame({\n",
        "        'Model': ['RAG Pipeline', 'Baseline'],\n",
        "        'Toplam Inference Time (s)': [\n",
        "            f\"{rag_inference:.2f}\" if isinstance(rag_inference, (int, float)) else rag_inference,\n",
        "            f\"{baseline_inference:.2f}\" if isinstance(baseline_inference, (int, float)) else baseline_inference\n",
        "        ],\n",
        "        'Avg Time per Question (s)': [\n",
        "            f\"{rag_inference/len(questions):.2f}\" if isinstance(rag_inference, (int, float)) else 'N/A',\n",
        "            f\"{baseline_inference/len(questions):.2f}\" if isinstance(baseline_inference, (int, float)) else 'N/A'\n",
        "        ],\n",
        "        'Memory Usage (MB)': [\n",
        "            f\"{rag_mem:.2f}\" if isinstance(rag_mem, (int, float)) else rag_mem,\n",
        "            f\"{baseline_mem:.2f}\" if isinstance(baseline_mem, (int, float)) else baseline_mem\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PERFORMANS METRİKLERİ KARŞILAŞTIRMASI (RAG vs BASELINE)\")\n",
        "    print(\"=\"*80)\n",
        "    print(perf_comparison.to_string(index=False))\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    models = ['RAG Pipeline', 'Baseline']\n",
        "\n",
        "    inference_times = [\n",
        "        rag_inference if isinstance(rag_inference, (int, float)) else 0,\n",
        "        baseline_inference if isinstance(baseline_inference, (int, float)) else 0\n",
        "    ]\n",
        "    memory_usage = [\n",
        "        rag_mem if isinstance(rag_mem, (int, float)) else 0,\n",
        "        baseline_mem if isinstance(baseline_mem, (int, float)) else 0\n",
        "    ]\n",
        "\n",
        "    # Inference Time\n",
        "    axes[0].bar(models, inference_times, color=['green', 'blue'], alpha=0.7)\n",
        "    axes[0].set_ylabel('Toplam Süre (saniye)', fontsize=11)\n",
        "    axes[0].set_title('Inference Time Karşılaştırması', fontsize=12, fontweight='bold')\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for i, (model, time) in enumerate(zip(models, inference_times)):\n",
        "        axes[0].text(i, time + max(inference_times)*0.02, f\"{time:.1f}s\", ha='center', fontsize=10)\n",
        "\n",
        "    # Memory Usage\n",
        "    axes[1].bar(models, memory_usage, color=['green', 'blue'], alpha=0.7)\n",
        "    axes[1].set_ylabel('Memory Kullanımı (MB)', fontsize=11)\n",
        "    axes[1].set_title('Memory Usage Karşılaştırması', fontsize=12, fontweight='bold')\n",
        "    axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for i, (model, mem) in enumerate(zip(models, memory_usage)):\n",
        "        axes[1].text(i, mem + abs(max(memory_usage, key=abs))*0.02, f\"{mem:.1f}MB\", ha='center', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('results/performance_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nGrafik kaydedildi: results/performance_comparison.png\")\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"UYARI: Bazı değişkenler tanımlı değil. Lütfen önce cell-29 ve cell-35'i çalıştırın.\")\n",
        "    print(f\"Hata: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jWNxQX_kaP-"
      },
      "source": [
        "---\n",
        "## 7. Performans Değerlendirme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQyZDDBdkaP-"
      },
      "source": [
        "### 7.1 BLEU ve ROUGE Metrikleri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acw0yq2ZkaP-"
      },
      "outputs": [],
      "source": [
        "evaluator = MetricsEvaluator()\n",
        "\n",
        "comparison = evaluator.compare_models(\n",
        "    rag_results=rag_results,\n",
        "    baseline_results=baseline_results,\n",
        "    ground_truth=test_df\n",
        ")\n",
        "\n",
        "evaluator.print_comparison(comparison)\n",
        "\n",
        "evaluator.save_results(comparison, \"results/rag_vs_baseline.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7qpWr98kaP-"
      },
      "source": [
        "### 7.2 Sonuçları Görselleştir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NejWVG2-kaP_"
      },
      "outputs": [],
      "source": [
        "metrics = ['bleu', 'rouge1', 'rouge2', 'rougeL']\n",
        "rag_scores = [comparison['rag'][m] for m in metrics]\n",
        "baseline_scores = [comparison['baseline'][m] for m in metrics]\n",
        "\n",
        "x = range(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.bar([i - width/2 for i in x], rag_scores, width, label='RAG', color='green')\n",
        "ax.bar([i + width/2 for i in x], baseline_scores, width, label='Baseline', color='blue')\n",
        "\n",
        "ax.set_xlabel('Metrikler', fontsize=12)\n",
        "ax.set_ylabel('Skor', fontsize=12)\n",
        "ax.set_title('RAG vs Baseline Performans Karşılaştırması', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([m.upper() for m in metrics])\n",
        "ax.legend()\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/rag_vs_baseline.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Grafik kaydedildi: results/rag_vs_baseline.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRsMS_n7kaP_"
      },
      "source": [
        "---\n",
        "## 8. Hiperparametre Optimizasyonu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Vcf6m9WkaP_"
      },
      "source": [
        "### 8.1 Grid Search Parametreleri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9Z7GlS790ev"
      },
      "outputs": [],
      "source": [
        "##NOT BU KISIMDA 2 WORKER'LI PARALLEL RUN ÇALISIYOR. T4 MAKİNEDE ZORLANDI VE HATA ALDI, A100 İLE DEVAM ETMEM GEREKTİ. RESTART EDIP A100 ile devam ettim.\n",
        "\n",
        "PARENT_SIZES = [2048, 4096]\n",
        "CHILD_SIZES = [512, 1024]\n",
        "TEMPERATURES = [0.1, 0.3]\n",
        "CHUNK_OVERLAPS = [0, 100, 200]\n",
        "\n",
        "runner = ExperimentRunner(\n",
        "    book_path=data_paths['book'],\n",
        "    test_questions_path=data_paths['test'],\n",
        "    results_dir=\"results/experiments_v2\"\n",
        ")\n",
        "\n",
        "print(\"Grid search başlatılıyor...\")\n",
        "print(f\"Toplam index oluşturma: {len(PARENT_SIZES) * len(CHILD_SIZES) * len(CHUNK_OVERLAPS)} kere\")\n",
        "print(f\"Toplam deney sayısı: {len(PARENT_SIZES) * len(CHILD_SIZES) * len(TEMPERATURES) * len(CHUNK_OVERLAPS)} deney\")\n",
        "print(\"\\nNOT: Her index bir kere oluşturulup farklı temperature'lerle test edilecek\")\n",
        "\n",
        "all_results = runner.run_grid_search(\n",
        "    parent_sizes=PARENT_SIZES,\n",
        "    child_sizes=CHILD_SIZES,\n",
        "    temperatures=TEMPERATURES,\n",
        "    chunk_overlaps=CHUNK_OVERLAPS\n",
        ")\n",
        "\n",
        "runner.save_summary(all_results, summary_filename=\"experiment_summary_v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKNaULMYkaP_"
      },
      "source": [
        "### 8.2 En İyi Parametreleri Bul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OmFmp5wkaP_"
      },
      "outputs": [],
      "source": [
        "exp_df = pd.read_csv('results/experiments_v2/experiment_summary_v2.csv')\n",
        "\n",
        "baseline_row = pd.Series({\n",
        "    'parent_size': 'Baseline',\n",
        "    'child_size': 'Baseline',\n",
        "    'temperature': 0.5,\n",
        "    'chunk_overlap': 0,\n",
        "    'bleu': comparison['baseline']['bleu'],\n",
        "    'rouge1': comparison['baseline']['rouge1'],\n",
        "    'rouge2': comparison['baseline']['rouge2'],\n",
        "    'rougeL': comparison['baseline']['rougeL'],\n",
        "    'avg_question_time': f'{baseline_inference_time/len(questions):.2f}',\n",
        "    'inference_time': f'{baseline_inference_time:.2f}',\n",
        "    'memory_used_mb': f'{baseline_memory_used:.2f}',\n",
        "    'total_time': None\n",
        "})\n",
        "\n",
        "\n",
        "exp_df_final = pd.concat([exp_df, baseline_row.to_frame().T], ignore_index=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5wI35Fp7aoj"
      },
      "outputs": [],
      "source": [
        "exp_df_final.to_csv(\"grid_search_results.csv\", index=False)\n",
        "exp_df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2dsC4Np7apk"
      },
      "outputs": [],
      "source": [
        "exp_df_final = exp_df_final.copy()\n",
        "exp_df_final['experiment_name'] = [f\"exp_{i+1}\" for i in range(len(exp_df_final))]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metrics = ['bleu', 'rougeL']\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "\n",
        "for metric in metrics:\n",
        "    plt.plot(\n",
        "        exp_df_final['experiment_name'],\n",
        "        exp_df_final[metric],\n",
        "        marker='o',\n",
        "        linewidth=2,\n",
        "        label=metric.upper()\n",
        "    )\n",
        "\n",
        "\n",
        "    for x, y in zip(exp_df_final['experiment_name'], exp_df_final[metric]):\n",
        "        plt.text(\n",
        "            x, y + 0.15,\n",
        "            f\"{y:.2f}\",\n",
        "            fontsize=7,\n",
        "            rotation=45,\n",
        "            ha='center'\n",
        "        )\n",
        "\n",
        "plt.xlabel(\"Experiment\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"BLEU / ROUGE-L Comparison Across Experiments\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.savefig('results/rag_vs_baseline_param_opt.png', dpi=300, bbox_inches='tight')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPC7uPIXGJw6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
